{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9H-v3De-5OC"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_percentage_error,mean_absolute_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "# Add to imports\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "om9EOM1M-53P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"founder_data1.csv\").sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Define feature categories\n",
        "continuous_features = [\n",
        "    'number_of_roles', 'number_of_companies', 'industry_achievements', 'max_amount_raised',\n",
        "    'previous_orgs_max_num_founders', 'org_num_founders', 'num_acquisitions', 'max_acquisition_amount',\n",
        "    'max_ipo_amount_raised', 'repeat_ideal_days', 'press_media_coverage_count', 'experienced_funding_rounds'\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    'perseverance', 'risk_tolerance', 'vision', 'adaptability', 'personal_branding', 'education_level',\n",
        "    'education_institution', 'education_field_of_study', 'big_leadership', 'nasdaq_leadership',\n",
        "    'number_of_leadership_roles', 'big_tech_position', 'worked_at_consultancy', 'worked_at_bank',\n",
        "    'vc_experience', 'angel_experience', 'quant_experience', 'investor_quality_prior_startup',\n",
        "    'previous_startup_funding_experience','org_state','org_city','org_category_list','domain_expertise','skill_relevance'\n",
        "]\n",
        "\n",
        "binary_features = [\n",
        "    'professional_athlete', 'childhood_entrepreneurship', 'competitions', 'ten_thousand_hours_of_mastery',\n",
        "    'education_international_experience', 'education_extracurricular_involvement', 'education_awards_and_honors',\n",
        "    'being_lead_of_nonprofits', 'big_company_experience', 'nasdaq_company_experience', 'big_tech_experience',\n",
        "    'google_experience', 'facebook_meta_experience', 'microsoft_experience', 'amazon_experience', 'apple_experience',\n",
        "    'career_growth', 'moving_around', 'international_work_experience', 'worked_at_military', 'board_advisor_roles',\n",
        "    'tier_1_vc_experience', 'startup_experience', 'ceo_experience', 'ipo_experience', 'founder_of_nonprofit'\n",
        "]\n",
        "\n",
        "# Convert binary features\n",
        "for col in binary_features:\n",
        "    df[col] = df[col].map({'TRUE': 1, 'FALSE': 0}).fillna(0).astype(int)\n",
        "\n",
        "# Ensure numeric features\n",
        "df[continuous_features] = df[continuous_features].apply(pd.to_numeric, errors='coerce')\n",
        "print(len(df))\n",
        "# Target variable\n",
        "target = 'org_total_funding_usd'\n",
        "df = df.dropna(subset=target).reset_index(drop=True)\n",
        "df[target] = np.log1p(df[target])\n",
        "print(len(df))\n",
        "print(5198 in df.index)\n",
        "# Process text features\n",
        "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "df['combined_text'] = df['org_name'].fillna('') + \" \" + df['org_description'].fillna('')\n",
        "text_embeddings = text_model.encode(df['combined_text'])\n",
        "\n",
        "# Split data\n",
        "X = df[continuous_features + categorical_features + binary_features]\n",
        "y = df[target]\n",
        "text_data = text_embeddings\n",
        "\n",
        "X_train, X_test, y_train, y_test, text_train, text_test = train_test_split(\n",
        "    X, y, text_data, test_size=0.2, random_state=42,shuffle=True\n",
        ")\n",
        "\n",
        "# Preprocessing pipelines\n",
        "continuous_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "binary_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('continuous', continuous_pipeline, continuous_features),\n",
        "    ('categorical', categorical_pipeline, categorical_features),\n",
        "    ('binary', binary_pipeline, binary_features)\n",
        "])\n",
        "\n",
        "# First-level models\n",
        "model_1 = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "model_2 = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Train base models\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "train_pred1 = model_1.predict(X_train)\n",
        "train_pred2 = model_2.predict(X_train)\n",
        "test_pred1 = model_1.predict(X_test)\n",
        "test_pred2 = model_2.predict(X_test)\n",
        "\n",
        "# Combine predictions with text embeddings\n",
        "stacked_train = np.column_stack((train_pred1, train_pred2, text_train))\n",
        "stacked_test = np.column_stack((test_pred1, test_pred2, text_test))\n",
        "\n",
        "# Meta-model with text-enhanced features\n",
        "meta_model = LinearRegression()\n",
        "meta_model.fit(stacked_train, y_train)\n",
        "final_predictions = meta_model.predict(stacked_test)\n",
        "\n",
        "# Evaluation\n",
        "mape = mean_absolute_percentage_error(np.expm1(y_test), np.expm1(final_predictions))\n",
        "mae = mean_absolute_error(np.expm1(y_test), np.expm1(final_predictions))\n",
        "\n",
        "print(f\"Mean Absolute Percentage Error: {mape:.2f}\")\n",
        "print(f\"Mean Absolute Error: ${mae:,.2f}\")\n",
        "\n",
        "# Save predictions\n",
        "X_test = X_test.copy()\n",
        "X_test[\"predicted_funding_usd\"] = np.expm1(final_predictions)\n",
        "#X_test.to_csv(\"test_set_with_predictions.csv\", index=False)\n",
        "print(X_test.head())"
      ],
      "metadata": {
        "id": "WECJzDPt-8Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mae(model1, model2, meta_model, X, y_true, text_embeddings):\n",
        "    \"\"\"Calculate MAE for the entire stacked model including text embeddings\"\"\"\n",
        "    pred1 = model1.predict(X)\n",
        "    pred2 = model2.predict(X)\n",
        "    stacked_pred = np.column_stack((pred1, pred2, text_embeddings))\n",
        "    final_pred = meta_model.predict(stacked_pred)\n",
        "    return mean_absolute_error(np.expm1(y_true), np.expm1(final_pred))\n",
        "\n",
        "# Calculate baseline MAE with original features and text embeddings\n",
        "baseline_mae = calculate_mae(model_1, model_2, meta_model, X_test, y_test, text_test)\n",
        "print(f\"Baseline MAE: ${baseline_mae:,.2f}\")\n",
        "\n",
        "# Initialize feature importance storage\n",
        "feature_impact = {}\n",
        "\n",
        "# Calculate permutation importance for each feature\n",
        "for feature in X.columns:\n",
        "    # Create permuted test set\n",
        "    X_permuted = X_test.copy()\n",
        "    X_permuted[feature] = X_test[feature].sample(frac=1, random_state=42).values  # Shuffle feature values\n",
        "\n",
        "    # Calculate MAE with permuted feature\n",
        "    permuted_mae = calculate_mae(model_1, model_2, meta_model, X_permuted, y_test, text_test)\n",
        "\n",
        "    # Store importance as MAE increase ratio\n",
        "    feature_impact[feature] = (permuted_mae - baseline_mae) / baseline_mae * 100\n",
        "\n",
        "# Calculate importance for text embeddings\n",
        "text_permuted = np.random.permutation(text_test)\n",
        "text_mae = calculate_mae(model_1, model_2, meta_model, X_test, y_test, text_permuted)\n",
        "feature_impact['text_embeddings'] = (text_mae - baseline_mae) / baseline_mae * 100\n",
        "\n",
        "# Create sorted DataFrame of feature impacts\n",
        "impact_df = pd.DataFrame({\n",
        "    'Feature': feature_impact.keys(),\n",
        "    'Impact (%)': feature_impact.values()\n",
        "}).sort_values('Impact (%)', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nTop 10 Most Impactful Features:\")\n",
        "print(impact_df.head(10))\n",
        "\n",
        "print(\"\\nTop 10 Least Impactful Features:\")\n",
        "print(impact_df.tail(10))\n",
        "\n",
        "print(\"\\nTop 20 Features:\")\n",
        "print(impact_df.head(20))\n",
        "\n",
        "# Analyze text feature importance separately\n",
        "text_weights = meta_model.coef_[2:]  # First 2 are base model weights\n",
        "print(f\"\\nOverall text contribution: {np.abs(text_weights).sum()/np.abs(meta_model.coef_).sum():.1%}\")\n",
        "\n",
        "# Most influential embedding dimensions\n",
        "top_dims = np.argsort(-np.abs(text_weights))[:5]\n",
        "print(\"Top 5 impactful text embedding dimensions:\", top_dims)"
      ],
      "metadata": {
        "id": "TFpfsAp-_CJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[continuous_features + categorical_features + binary_features]\n",
        "y = df[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Rerun the model training and prediction steps\n",
        "# ... (rerun the model training code here)\n",
        "\n",
        "# Categorize funding function\n",
        "def categorize_funding(amount):\n",
        "    if amount < 1e6:\n",
        "        return \"100k-1M\"\n",
        "    elif amount < 1e7:\n",
        "        return \"1M-10M\"\n",
        "    elif amount < 1e8:\n",
        "        return \"10M-100M\"\n",
        "    elif amount < 1e9:\n",
        "        return \"100M-1B\"\n",
        "    else:\n",
        "        return \"1B+\"\n",
        "\n",
        "# Add predictions and funding category to the training set\n",
        "X_train_copy = X_train.copy()\n",
        "X_train_copy[\"predicted_funding_usd\"] = np.expm1(model_1.predict(X_train))\n",
        "X_train_copy[\"funding_category\"] = X_train_copy[\"predicted_funding_usd\"].apply(categorize_funding)\n",
        "X_train_copy[\"success\"] = df.loc[X_train.index, \"success\"]\n",
        "\n",
        "X_test_copy = X_test.copy()\n",
        "X_test_copy[\"predicted_funding_usd\"] = np.expm1(final_predictions)\n",
        "X_test_copy[\"funding_category\"] = X_test_copy[\"predicted_funding_usd\"].apply(categorize_funding)\n",
        "X_test_copy[\"success\"] = df.loc[X_test.index, \"success\"]\n",
        "\n",
        "# Calculate success probabilities using the training set\n",
        "def calculate_success_probability(df, success_column):\n",
        "    success_probabilities = df.groupby(\"funding_category\")[success_column].mean()\n",
        "    return success_probabilities\n",
        "\n",
        "success_probabilities = calculate_success_probability(X_train_copy, \"success\")\n",
        "\n",
        "print(\"Success probabilities by funding category (from training set):\")\n",
        "print(success_probabilities)\n",
        "\n",
        "# Now apply these probabilities to the test set\n",
        "X_test_copy = X_test.copy()\n",
        "X_test_copy[\"predicted_funding_usd\"] = np.expm1(final_predictions)\n",
        "X_test_copy[\"funding_category\"] = X_test_copy[\"predicted_funding_usd\"].apply(categorize_funding)\n",
        "X_test_copy[\"success\"] = df.loc[X_test.index, \"success\"]\n",
        "X_test_copy[\"predicted_success_prob\"] = X_test_copy[\"funding_category\"].map(success_probabilities)\n",
        "\n",
        "# Calculate accuracy of the success prediction on the test set\n",
        "def calculate_accuracy(df, prob_threshold=0.5):\n",
        "    df[\"predicted_success\"] = df[\"predicted_success_prob\"] > prob_threshold\n",
        "    accuracy = (df[\"predicted_success\"] == df[\"success\"]).mean()\n",
        "    return accuracy\n",
        "\n",
        "accuracy = calculate_accuracy(X_test_copy)\n",
        "print(f\"\\nAccuracy of success prediction on test set: {accuracy:.2f}\")\n",
        "\n",
        "# Calculate additional metrics\n",
        "true_positives = ((X_test_copy[\"predicted_success\"] == 1) & (X_test_copy[\"success\"] == 1)).sum()\n",
        "false_positives = ((X_test_copy[\"predicted_success\"] == 1) & (X_test_copy[\"success\"] == 0)).sum()\n",
        "total_positives = X_test_copy[\"success\"].sum()\n",
        "test_set_length = len(X_test_copy)\n",
        "\n",
        "print(\"\\nAdditional Metrics:\")\n",
        "print(f\"Total number of positives predicted correctly (True Positives): {true_positives}\")\n",
        "print(f\"Total number of positives incorrectly predicted (False Positives): {false_positives}\")\n",
        "print(f\"Total number of actual positives in test set: {total_positives}\")\n",
        "print(f\"Length of test data: {test_set_length}\")\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "recall = true_positives / total_positives if total_positives > 0 else 0\n",
        "\n",
        "print(f\"\\nPrecision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n"
      ],
      "metadata": {
        "id": "LLLsmBIP_HWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[continuous_features + categorical_features + binary_features]  # Remove 'org_total_funding_usd'\n",
        "y_funding = df[target]\n",
        "y_success = df[\"success\"]\n",
        "\n",
        "# Process text features\n",
        "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "df['combined_text'] = df['org_name'].fillna('') + \" \" + df['org_description'].fillna('')\n",
        "text_embeddings = text_model.encode(df['combined_text'].tolist())\n",
        "\n",
        "# Split data while maintaining alignment\n",
        "X_train, X_test, y_train_funding, y_test_funding, y_train_success, y_test_success, text_train, text_test = train_test_split(\n",
        "    X, y_funding, y_success, text_embeddings, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ... (keep preprocessing and stacking model code)\n",
        "\n",
        "# Generate funding predictions CORRECTLY\n",
        "train_funding_pred = meta_model.predict(\n",
        "    np.column_stack((model_1.predict(X_train), model_2.predict(X_train), text_train))\n",
        ")\n",
        "test_funding_pred = meta_model.predict(\n",
        "    np.column_stack((model_1.predict(X_test), model_2.predict(X_test), text_test))\n",
        ")\n",
        "\n",
        "# Prepare logistic regression data CORRECTLY\n",
        "X_train_logistic = pd.DataFrame({'predicted_funding': train_funding_pred})\n",
        "X_test_logistic = pd.DataFrame({'predicted_funding': test_funding_pred})\n",
        "print(X_train_logistic.head())\n",
        "print(X_test_logistic.head())\n",
        "\n",
        "# Rest remains the same\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_logistic, y_train_success)\n",
        "\n",
        "success_probabilities = log_reg.predict_proba(X_test_logistic)[:, 1]\n",
        "success_predictions = (success_probabilities >= 0.75).astype(int)\n",
        "print(success_probabilities)\n",
        "\n",
        "# Evaluation code remains unchanged\n",
        "print(\"\\nLogistic Regression Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_success, success_predictions):.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_success, success_predictions))\n",
        "\n",
        "cm = confusion_matrix(y_test_success, success_predictions)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\nDetailed Success Prediction Metrics:\")\n",
        "print(f\"True Positives (Correct Successes): {tp}\")\n",
        "print(f\"False Positives (Incorrect Successes): {fp}\")\n",
        "print(f\"True Negatives (Correct Non-Successes): {tn}\")\n",
        "print(f\"False Negatives (Incorrect Non-Successes): {fn}\")\n",
        "print(f\"Total Actual Positives: {tp + fn}\")\n",
        "print(f\"Total Test Samples: {len(X_test)}\")\n",
        "\n",
        "# Ensure we're using the TEST dataframe copy\n",
        "X_test = X_test.copy()\n",
        "X_test[\"predicted_success\"] = success_predictions\n",
        "X_test[\"success_probability\"] = success_probabilities\n",
        "X_test[\"predicted_funding_usd\"] = np.expm1(test_funding_pred)  # Convert back from log scale\n",
        "\n",
        "# Add text embedding analysis\n",
        "text_weights = meta_model.coef_[2:]  # First 2 are base model weights\n",
        "print(f\"\\nText contribution in funding prediction: {np.abs(text_weights).sum()/np.abs(meta_model.coef_).sum():.1%}\")\n",
        "\n",
        "# Most influential embedding dimensions for funding prediction\n",
        "top_dims = np.argsort(-np.abs(text_weights))[:5]\n",
        "print(\"Top 5 impactful text embedding dimensions for funding:\", top_dims)\n",
        "\n",
        "# Analyze text impact on success prediction\n",
        "text_success_impact = log_reg.coef_[0] * meta_model.coef_[2:]\n",
        "print(f\"\\nText contribution in success prediction: {np.abs(text_success_impact).sum()/np.abs(log_reg.coef_[0] * meta_model.coef_).sum():.1%}\")\n",
        "\n",
        "# Most influential embedding dimensions for success prediction\n",
        "top_success_dims = np.argsort(-np.abs(text_success_impact))[:5]\n",
        "print(\"Top 5 impactful text embedding dimensions for success:\", top_success_dims)"
      ],
      "metadata": {
        "id": "MEKqcN8A_QdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}